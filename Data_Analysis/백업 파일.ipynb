{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dataframes = pd.read_pickle('recom_data/user_category_dummy3.pkl')\n",
    "\n",
    "df_counts = dataframes['view_counts']\n",
    "df_users = dataframes['users']\n",
    "\n",
    "with open('recom_data/category_dict.pkl', 'rb') as handle:\n",
    "    category_dict = pickle.load(handle)\n",
    "\n",
    "item_similarity = pd.read_pickle('recom_data/item_similarity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_matrix = df_counts.pivot(index='user', columns='category', values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category     sum\n",
      "0        스마트 밴드  1.4500\n",
      "1        스마트 워치  1.2500\n",
      "2       남성 캐주얼화  1.1500\n",
      "3        남성 운동화  1.1500\n",
      "4        남성용 모자  1.1000\n",
      "..          ...     ...\n",
      "144  기타 스케이트 용품  0.2125\n",
      "145       남성 향수  0.2125\n",
      "146         전동휠  0.2000\n",
      "147          목공  0.2000\n",
      "148  기타 헤어 스타일링  0.1375\n",
      "\n",
      "[149 rows x 2 columns]\n",
      "       category     sum\n",
      "7         남성 하의  2.9875\n",
      "5         남성 상의  2.0625\n",
      "0        스마트 밴드  1.4500\n",
      "1        스마트 워치  1.2500\n",
      "2       남성 캐주얼화  1.1500\n",
      "..          ...     ...\n",
      "144  기타 스케이트 용품  0.2125\n",
      "145       남성 향수  0.2125\n",
      "146         전동휠  0.2000\n",
      "147          목공  0.2000\n",
      "148  기타 헤어 스타일링  0.1375\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def transform_gender(gender):\n",
    "    return 'M' if (gender == '남자') else 'F'\n",
    "\n",
    "def transform_age(age):\n",
    "    return (age // 10) * 10\n",
    "\n",
    "def recom_simple1(user, n_items=20):\n",
    "    user_gender = transform_gender(user['gender'])\n",
    "    user_age = transform_age(user['age'])\n",
    "\n",
    "    mask_gender = df_users['gender'] == user_gender\n",
    "    mask_age = df_users['age'] == user_age\n",
    "\n",
    "    user_group = df_users[mask_gender & mask_age]\n",
    "    \n",
    "    mask_group = new_df['user'].isin(user_group['user'])\n",
    "    view_df = new_df[mask_group]\n",
    "    \n",
    "    grouped_view_df = (view_df.groupby(['category'])['count']\n",
    "                       .agg(['sum'])\n",
    "                       .sort_values(by=['sum'], ascending=False)\n",
    "                       .apply(lambda x: x / len(view_df['user'].unique()))\n",
    "                       .reset_index())\n",
    "    \n",
    "    return grouped_view_df\n",
    "\n",
    "def recom_simple2(user, n_items=20):    \n",
    "    grouped_view_df = recom_simple1(user, n_items=n_items)\n",
    "    \n",
    "    for tester_view in tester_view_data:\n",
    "        if tester_view[1] in grouped_view_df['category'].values:\n",
    "            grouped_view_df.loc[grouped_view_df.category==tester_view[1], 'sum'] += tester_view[2]\n",
    "        else:\n",
    "            \n",
    "            grouped_view_df = grouped_view_df.append({'category': tester_view[1], 'sum': tester_view[2]}, ignore_index=True)\n",
    "    \n",
    "    grouped_view_df = grouped_view_df.sort_values(by=['sum'], ascending=False)\n",
    "    return grouped_view_df\n",
    "\n",
    "tester_view_data = [[2, 11, 1], [2, 12, 2], [2, 1, 1]]\n",
    "tester_df = pd.DataFrame(tester_view_data, columns=['user', 'category', 'count'])\n",
    "new_df = df_counts.append(tester_df)\n",
    "\n",
    "tester1 = {'user_id': 1, 'gender': '남자', 'age': 27}\n",
    "\n",
    "result1 = recom_simple1(tester1)\n",
    "result1['category'] = result1['category'].apply(lambda x: category_dict[x])\n",
    "print(result1)\n",
    "\n",
    "tester2 = {'user_id': 2, 'gender': '남자', 'age': 22}\n",
    "\n",
    "result2 = recom_simple2(tester2)\n",
    "result2['category'] = result2['category'].apply(lambda x: category_dict[x])\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "남자 10\n",
      "    category     sum\n",
      "0     남성 운동화  1.2250\n",
      "1     남성용 가방  1.2125\n",
      "2      남성 하의  1.1875\n",
      "3        야구공  1.1375\n",
      "4    남성 캐주얼화  1.1250\n",
      "..       ...     ...\n",
      "89        잡지  0.0875\n",
      "90     축구 의류  0.0875\n",
      "91       탁구대  0.0875\n",
      "92  기타 축구 용품  0.0875\n",
      "93     축구 가방  0.0750\n",
      "\n",
      "[94 rows x 2 columns]\n",
      "남자 20\n",
      "       category     sum\n",
      "0        스마트 밴드  1.4500\n",
      "1        스마트 워치  1.2500\n",
      "2       남성 캐주얼화  1.1500\n",
      "3        남성 운동화  1.1500\n",
      "4        남성용 모자  1.1000\n",
      "..          ...     ...\n",
      "144  기타 스케이트 용품  0.2125\n",
      "145       남성 향수  0.2125\n",
      "146         전동휠  0.2000\n",
      "147          목공  0.2000\n",
      "148  기타 헤어 스타일링  0.1375\n",
      "\n",
      "[149 rows x 2 columns]\n",
      "남자 30\n",
      "      category       sum\n",
      "0       스마트 밴드  1.650000\n",
      "1       스마트 워치  1.466667\n",
      "2          키보드  1.216667\n",
      "3     기타 음향 가전  1.200000\n",
      "4          청소기  1.183333\n",
      "..         ...       ...\n",
      "143         헹거  0.266667\n",
      "144     아동 서랍장  0.250000\n",
      "145  기타 DIY 자재  0.250000\n",
      "146     주방 수납장  0.216667\n",
      "147      책상/의자  0.216667\n",
      "\n",
      "[148 rows x 2 columns]\n",
      "남자 40\n",
      "      category       sum\n",
      "0     문학/과학/경영  1.566667\n",
      "1        문구 용품  1.433333\n",
      "2       소설/만화책  1.350000\n",
      "3        사무 용품  1.333333\n",
      "4     여행/취미/레저  1.300000\n",
      "..         ...       ...\n",
      "189         걸레  0.250000\n",
      "190      오일 용품  0.233333\n",
      "191  기타 자동차 용품  0.233333\n",
      "192      세차 용품  0.233333\n",
      "193        바구니  0.200000\n",
      "\n",
      "[194 rows x 2 columns]\n",
      "남자 50\n",
      "        category       sum\n",
      "0          문구 용품  1.166667\n",
      "1          기타 문구  1.083333\n",
      "2       문학/과학/경영  1.066667\n",
      "3             잡지  1.033333\n",
      "4           전공서적  1.016667\n",
      "..           ...       ...\n",
      "141          탁구대  0.333333\n",
      "142       눈건강 용품  0.316667\n",
      "143     냉온/찜질 용품  0.300000\n",
      "144           TV  0.283333\n",
      "145  물리치료/저주파 용품  0.216667\n",
      "\n",
      "[146 rows x 2 columns]\n",
      "남자 60\n",
      "        category       sum\n",
      "0         눈건강 용품  1.700000\n",
      "1          의료 용품  1.600000\n",
      "2        당뇨관리 용품  1.516667\n",
      "3        재활운동 용품  1.500000\n",
      "4    물리치료/저주파 용품  1.433333\n",
      "..           ...       ...\n",
      "124         배드민턴  0.316667\n",
      "125        기타 문구  0.316667\n",
      "126       소설/만화책  0.300000\n",
      "127       유아동 도서  0.283333\n",
      "128       예술/디자인  0.266667\n",
      "\n",
      "[129 rows x 2 columns]\n",
      "남자 70\n",
      "      category       sum\n",
      "0     문학/과학/경영  1.216667\n",
      "1       유아동 도서  1.150000\n",
      "2           잡지  1.016667\n",
      "3       예술/디자인  1.016667\n",
      "4     여행/취미/레저  0.983333\n",
      "..         ...       ...\n",
      "119  기타 DIY 자재  0.416667\n",
      "120      안전 용품  0.400000\n",
      "121         시계  0.366667\n",
      "122      난방 기구  0.350000\n",
      "123     눈건강 용품  0.250000\n",
      "\n",
      "[124 rows x 2 columns]\n",
      "여자 10\n",
      "       category     sum\n",
      "0         학습/교육  1.3500\n",
      "1         공용 모자  1.3125\n",
      "2        소설/만화책  1.3000\n",
      "3         여성 하의  1.2875\n",
      "4        여성용 모자  1.2750\n",
      "..          ...     ...\n",
      "121  블러셔, 하이라이터  0.2875\n",
      "122      손발톱정리기  0.2500\n",
      "123          잡지  0.2500\n",
      "124      예술/디자인  0.2125\n",
      "125      네일 리무버  0.1750\n",
      "\n",
      "[126 rows x 2 columns]\n",
      "여자 20\n",
      "        category     sum\n",
      "0       헤어밴드/핀/끈  1.4500\n",
      "1             반지  1.4000\n",
      "2          여성 장갑  1.4000\n",
      "3        기타 액세서리  1.4000\n",
      "4          여성 구두  1.3500\n",
      "..           ...     ...\n",
      "180           기타  0.2125\n",
      "181          DVD  0.2000\n",
      "182  기타 스키/보드 용품  0.2000\n",
      "183          볼링공  0.2000\n",
      "184         블루레이  0.1875\n",
      "\n",
      "[185 rows x 2 columns]\n",
      "여자 30\n",
      "      category       sum\n",
      "0       여성용 모자  1.816667\n",
      "1           잡지  1.783333\n",
      "2       여성 실내화  1.683333\n",
      "3        공용 시계  1.683333\n",
      "4      기타 액세서리  1.666667\n",
      "..         ...       ...\n",
      "129  기타 자전거 용품  0.283333\n",
      "130         랜턴  0.266667\n",
      "131   기타 등산 용품  0.250000\n",
      "132      스키 장비  0.233333\n",
      "133      등산 의류  0.233333\n",
      "\n",
      "[134 rows x 2 columns]\n",
      "여자 40\n",
      "     category       sum\n",
      "0          잡지  1.683333\n",
      "1    여행/취미/레저  1.550000\n",
      "2       화방 용품  1.550000\n",
      "3       기타 도서  1.500000\n",
      "4    문학/과학/경영  1.416667\n",
      "..        ...       ...\n",
      "157        랜턴  0.266667\n",
      "158        텐트  0.250000\n",
      "159  자전거 안전용품  0.250000\n",
      "160     요가 용품  0.233333\n",
      "161    자전거 부품  0.100000\n",
      "\n",
      "[162 rows x 2 columns]\n",
      "여자 50\n",
      "    category       sum\n",
      "0         수예  1.150000\n",
      "1      기타 침구  1.050000\n",
      "2         조명  1.016667\n",
      "3         기타  1.000000\n",
      "4        홈데코  1.000000\n",
      "..       ...       ...\n",
      "232    샤워 용품  0.233333\n",
      "233  의류관리 용품  0.233333\n",
      "234   눈건강 용품  0.233333\n",
      "235    헤어 케어  0.233333\n",
      "236   관상어 사료  0.183333\n",
      "\n",
      "[237 rows x 2 columns]\n",
      "여자 60\n",
      "     category       sum\n",
      "0       여성 상의  1.383333\n",
      "1       여성 잠옷  1.366667\n",
      "2       남성 상의  1.266667\n",
      "3       남성 하의  1.266667\n",
      "4          기타  1.200000\n",
      "..        ...       ...\n",
      "129    아동 서랍장  0.266667\n",
      "130  기타 침실 가구  0.250000\n",
      "131       카페트  0.250000\n",
      "132     아동 책장  0.233333\n",
      "133        액자  0.200000\n",
      "\n",
      "[134 rows x 2 columns]\n",
      "여자 70\n",
      "      category       sum\n",
      "0        기타 도서  1.350000\n",
      "1       예술/디자인  1.266667\n",
      "2         전공서적  1.083333\n",
      "3           잡지  1.066667\n",
      "4        필기 용품  1.016667\n",
      "..         ...       ...\n",
      "174        화장지  0.250000\n",
      "175     고양이 사료  0.250000\n",
      "176  기타 관상어 용품  0.233333\n",
      "177         식기  0.233333\n",
      "178     강아지 간식  0.183333\n",
      "\n",
      "[179 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "testers = [{'gender': '남자', 'age': 10}, {'gender': '남자', 'age': 20}, {'gender': '남자', 'age': 30}, {'gender': '남자', 'age': 40}, {'gender': '남자', 'age': 50}, {'gender': '남자', 'age': 60}, {'gender': '남자', 'age': 70},\n",
    "           {'gender': '여자', 'age': 10}, {'gender': '여자', 'age': 20}, {'gender': '여자', 'age': 30}, {'gender': '여자', 'age': 40}, {'gender': '여자', 'age': 50}, {'gender': '여자', 'age': 60}, {'gender': '여자', 'age': 70}]\n",
    "\n",
    "for test in testers:\n",
    "    print(test['gender'], test['age'])\n",
    "    result1 = recom_simple1(test)\n",
    "    result1['category'] = result1['category'].apply(lambda x: category_dict[x])\n",
    "    print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x = df_counts.copy()\n",
    "y = df_counts['user']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE 계산해주는 함수\n",
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true)- np.array(y_pred)) ** 2))\n",
    "\n",
    "# 모델별 RMSE 계산\n",
    "def score(model):\n",
    "    id_pairs = zip(x_test['user'], x_test['category'])\n",
    "    y_pred = np.array([model(user, category) for (user, category) in id_pairs])\n",
    "    y_true = np.array(x_test['count'])\n",
    "    return RMSE(y_true, y_pred)\n",
    "\n",
    "train_view_matrix = x_train.pivot(index='user', columns='category', values='count')\n",
    "view_matrix_t = np.transpose(train_view_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_all_model(user, category):\n",
    "    try:\n",
    "        pred_viewing = train_mean[category]\n",
    "    except:\n",
    "        pred_viewing = 0\n",
    "    return pred_viewing\n",
    "\n",
    "user_count = len(x_train['user'].unique())\n",
    "train_mean = x_train.groupby(['category'])['count'].sum().apply(lambda x: x/user_count)\n",
    "\n",
    "# print(score(simple_all_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IBCF_model(user, category):\n",
    "    if category in item_similarity:\n",
    "        sim_scores = item_similarity[category]\n",
    "        user_viewing = view_matrix_t[user]\n",
    "        non_viewing_idx = user_viewing[user_viewing.isnull()].index\n",
    "        user_viewing = user_viewing.dropna()\n",
    "        sim_scores = sim_scores.drop(non_viewing_idx)\n",
    "        pred_viewing = np.dot(sim_scores, user_viewing) / sim_scores.sum()\n",
    "    else:\n",
    "        pred_viewing = 0\n",
    "    return pred_viewing\n",
    "\n",
    "# print(score(IBCF_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = {\n",
    "    'gender': '여성',\n",
    "    'age': 26\n",
    "}\n",
    "\n",
    "test_user_view_data = [\n",
    "    [1, 1, 4],\n",
    "    [1, 2, 2],\n",
    "    [1, 3, 2],\n",
    "    [1, 4, 1],\n",
    "    [1, 31, 1],\n",
    "    [1, 32, 1],\n",
    "    [1, 33, 3]\n",
    "]\n",
    "\n",
    "temp_user_df = pd.DataFrame(test_user_view_data, columns=['user', 'category', 'count'])\n",
    "new_df = df_counts.append(temp_user_df)\n",
    "\n",
    "view_matrix = new_df.pivot(index='user', columns='category', values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['남성 캐주얼화', '남성 잠옷', '남성 운동화', '여성 기능성화', '남성 언더웨어', '기타', '장지갑', '남성 하의', '중지갑', '남성 구두']\n"
     ]
    }
   ],
   "source": [
    "def recom_category(user_id, n_items=20):\n",
    "    for category in view_matrix:\n",
    "        user_category.loc[category] = IBCF_model(user_id, category)\n",
    "    category_sort = user_category.sort_values(ascending=False)[:n_items]\n",
    "    return category_sort\n",
    "\n",
    "def IBCF_model(user, category):\n",
    "    if category in item_similarity:\n",
    "        sim_scores = item_similarity[category]\n",
    "        user_viewing = user_category.T\n",
    "        non_viewing_idx = user_viewing[user_viewing.isnull()].index\n",
    "        user_viewing = user_viewing.dropna()\n",
    "        sim_scores = sim_scores.drop(non_viewing_idx)\n",
    "        pred_viewing = np.dot(sim_scores, user_viewing) / sim_scores.sum()\n",
    "    else:\n",
    "        pred_viewing = 0\n",
    "    return pred_viewing\n",
    "\n",
    "user_category = view_matrix.loc[90081].copy()\n",
    "\n",
    "pred_user_category = recom_category(user_id=90081, n_items=10)\n",
    "recommend_category = pd.DataFrame(pred_user_category).index.map(lambda x: category_dict[x])\n",
    "print(list(recommend_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['기타 바디 케어', '두피 케어', '애프터선', '남성 향수', '남성용 시계', '기타 헤어 스타일링', '남성 장갑', '남성 벨트', '기타 마스크/팩', '손수건']\n"
     ]
    }
   ],
   "source": [
    "item_similarity2 = pd.read_pickle('recom_data/item_similarity_pearson.pkl')\n",
    "\n",
    "def recom_category(user_id, n_items=20):\n",
    "    for category in view_matrix:\n",
    "        user_category.loc[category] = IBCF_model(user_id, category)\n",
    "    category_sort = user_category.sort_values(ascending=False)[:n_items]\n",
    "    return category_sort\n",
    "\n",
    "def IBCF_model(user, category):\n",
    "    if category in item_similarity2:\n",
    "        sim_scores = item_similarity2[category]\n",
    "        user_viewing = user_category.T\n",
    "        non_viewing_idx = user_viewing[user_viewing.isnull()].index\n",
    "        user_viewing = user_viewing.dropna()\n",
    "        sim_scores = sim_scores.drop(non_viewing_idx)\n",
    "        pred_viewing = np.dot(sim_scores, user_viewing) / sim_scores.sum()\n",
    "    else:\n",
    "        pred_viewing = 0\n",
    "    return pred_viewing\n",
    "\n",
    "user_category = view_matrix.loc[90081].copy()\n",
    "\n",
    "pred_user_category = recom_category(user_id=90081, n_items=10)\n",
    "recommend_category = pd.DataFrame(pred_user_category).index.map(lambda x: category_dict[x])\n",
    "print(list(recommend_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524250755100349\n"
     ]
    }
   ],
   "source": [
    "def recom_simple3(user, n_items=423):    \n",
    "    grouped_view_df = recom_simple1(user, n_items=423)\n",
    "    \n",
    "    for tester_view in test_user_view_data:\n",
    "        if tester_view[1] in grouped_view_df['category'].values:\n",
    "            grouped_view_df.loc[grouped_view_df.category==tester_view[1], 'sum'] += tester_view[2]\n",
    "        else:\n",
    "            \n",
    "            grouped_view_df = grouped_view_df.append({'category': tester_view[1], 'sum': tester_view[2]}, ignore_index=True)\n",
    "    \n",
    "    grouped_view_df = grouped_view_df.sort_values(by=['sum'], ascending=False)\n",
    "    return grouped_view_df\n",
    "\n",
    "\n",
    "pred1_user_category = recom_simple3(test_user, n_items=423)['sum']\n",
    "true1_user_category = pd.DataFrame(test_user_view_data, columns=['user', 'category', 'sum']).drop(['user'], axis=1).set_index('category')['sum']\n",
    "merged = pd.merge(pred1_user_category, true1_user_category, left_index=True, right_index=True, how='outer', suffixes=['_pred', '_true']).fillna(0)\n",
    "print(RMSE(merged['sum_pred'], merged['sum_true']))\n",
    "# 기존 그룹화 추천의 경우 데이터가 없는 경우는 Error가 낮게 나올 수밖에 없음\n",
    "# 모델과 에러가 큰 차이가 안 날 경우 전환하는 걸로 얘기해야 될 듯....\n",
    "\n",
    "# user = {\n",
    "#     'user_id': 1,\n",
    "#     'age': 12,\n",
    "#     'gender': '남성'\n",
    "# }\n",
    "# true2_user_category = view_matrix.loc[user['user_id']].fillna(0)\n",
    "# pred2_user_category = recom_simple3(user=user, n_items=423)\n",
    "# print(true2_user_category, pred2_user_category)\n",
    "# print(RMSE(pred2_user_category, true2_user_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF():\n",
    "    def __init__(self, ratings, K, alpha, beta, iterations, verbose=True):\n",
    "        \"\"\"\n",
    "            K: 잠재요인(latent factor)의 수\n",
    "            alpha: 학습률\n",
    "            beta: 정규화 계수\n",
    "            iterations: SGD의 계산을 할 때의 반복 횟수\n",
    "            verbose: SGD의 중간 학습과정 출력 여부\n",
    "        \"\"\"\n",
    "        self.R = np.array(ratings)\n",
    "        \n",
    "        item_id_index = []\n",
    "        index_item_id = []\n",
    "        for i, one_id in enumerate(ratings):\n",
    "            item_id_index.append([one_id, i])\n",
    "            index_item_id.append([i, one_id])\n",
    "        self.item_id_index = dict(item_id_index)\n",
    "        self.index_item_id = dict(index_item_id)\n",
    "        user_id_index = []\n",
    "        index_user_id = []\n",
    "        for i, one_id in enumerate(ratings.T):\n",
    "            user_id_index.append([one_id, i])\n",
    "            index_user_id.append([i, one_id])\n",
    "        self.user_id_index = dict(user_id_index)\n",
    "        self.index_user_id = dict(index_user_id)\n",
    "        \n",
    "        self.num_users, self.num_items = np.shape(self.R)\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def rmse(self):\n",
    "        \"\"\"\n",
    "            현재의 P행렬과 Q행렬을 가지고 Root Mean Squared Error(RMSE)를 계산\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        # R에서 평점이 있는 (0이 아닌) 요소의 인덱스를 저장\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        for x, y in zip(xs, ys):\n",
    "            # 평점이 존재하는 요소 (사용자 x, 아이템 y) 각각에 대해 아래 코드를 실행\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            # 사용자 x, 아이템 y에 대한 평점 예측치를 계산\n",
    "            self.predictions.append(prediction)\n",
    "            # 예측값을 리스트에 추가\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "            # 실제값과 예측값의 차이를 계산해서 오차값 리스트에 추가\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "        # errors를 사용해서 RMSE를 계산\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            정해진 횟수만큼 반복하며 P, Q, bu, bd 값을 업데이트하는 함수\n",
    "        \"\"\"\n",
    "        # Initializing user-feature and movie-feature matrix\n",
    "        # P, Q 행렬을 임의의 값으로 채움\n",
    "        # 평균이 0, 표준편차가 1/K인 정규분포를 갖는 난수로 초기화\n",
    "        self.P = np.random.normal(scale=10./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=10./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initializing the bias terms\n",
    "        # 사용자 평가 경향(bu)를 0으로 초기화 / 크기는 사용자 수 (num_users)\n",
    "        # 아이템 평가 경향(bd)를 0으로 초기화 / 크기는 아이템 수 (num_items)\n",
    "        # 전체 평균 b를 구해서 저장\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[self.R.nonzero()])\n",
    "        \n",
    "        # List of training samples\n",
    "        # 평점 행렬에서 값이 존재하는 요소의 인덱스들을 가져옴\n",
    "        # SGD를 적용할 대상 (평점이 있는 요소의 인덱스와 평점)을 리스트로 저장\n",
    "        rows, columns = self.R.nonzero()\n",
    "        self.samples = [(i, j, self.R[i,j]) for i, j in zip(rows, columns)]\n",
    "        \n",
    "        # Stochastic gardient descent for given number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            # samples를 임의로 섞음 => 어디서 시작하는지에 따라 수렴의 속도가 달라짐\n",
    "            # 매 반복마다 다양한 시작점에서 시작하기!\n",
    "            self.sgd()\n",
    "            # sgd를 실행 / P, Q, bu, bd가 업데이트 됨\n",
    "            rmse = self.rmse()\n",
    "            training_process.append((i+1, rmse))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"반복: {0}; Train RMSE = {1:0.4f}\".format(i+1, rmse))\n",
    "        return training_process\n",
    "    \n",
    "    def get_prediction (self, i, j):\n",
    "        \"\"\"\n",
    "            Rating prediction for user i and item j\n",
    "            사용자 i의 아이템 j에 대한 평점을 예측하는 함수\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "        \n",
    "    def get_one_prediction(self, user_id, item_id):\n",
    "        return self.get_prediction(self.user_id_index[user_id], self.item_id_index[item_id])\n",
    "    \n",
    "    def full_prediction(self):\n",
    "        return self.b + self.b_u[:, np.newaxis] + self.b_d[np.newaxis, :] + self.P.dot(self.Q.T)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "            Stochastic gardient descent to get optimized P and Q matrix\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # samples에 있는 각 사용자-아이템-평점 세트에 대해서 SGD를 적용\n",
    "            prediction = self.get_prediction(i, j)\n",
    "            # 사용자 i, 아이템 j에 대한 평점 예측치를 구함\n",
    "            e = (r - prediction)\n",
    "            # 실제 평점 r과 비교해서 오차를 구함\n",
    "            \n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_d[j] += self.alpha * (e - self.beta * self.b_d[j])\n",
    "            # 사용자 평가 경향, 아이템 평가 경향 업데이트\n",
    "            \n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i, :])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복: 10; Train RMSE = 0.8884\n",
      "반복: 20; Train RMSE = 0.8431\n",
      "반복: 30; Train RMSE = 0.8108\n",
      "반복: 40; Train RMSE = 0.7845\n",
      "반복: 50; Train RMSE = 0.7611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.9573017802630084),\n",
       " (2, 0.9469823239685168),\n",
       " (3, 0.9376099111593666),\n",
       " (4, 0.929013394184817),\n",
       " (5, 0.9210669673797167),\n",
       " (6, 0.9136783069930415),\n",
       " (7, 0.9067743077113968),\n",
       " (8, 0.9002892069116973),\n",
       " (9, 0.894177543851642),\n",
       " (10, 0.8883989255483337),\n",
       " (11, 0.882916388016239),\n",
       " (12, 0.8777023374561371),\n",
       " (13, 0.872728415816223),\n",
       " (14, 0.8679748404811775),\n",
       " (15, 0.8634216499596344),\n",
       " (16, 0.8590527222080991),\n",
       " (17, 0.8548521531755557),\n",
       " (18, 0.8508061023347225),\n",
       " (19, 0.8469020336052556),\n",
       " (20, 0.8431293744155139),\n",
       " (21, 0.8394779630569789),\n",
       " (22, 0.83593920368206),\n",
       " (23, 0.8325044089998904),\n",
       " (24, 0.8291668772306048),\n",
       " (25, 0.8259182784281832),\n",
       " (26, 0.8227523734950487),\n",
       " (27, 0.819664410430239),\n",
       " (28, 0.8166482030229743),\n",
       " (29, 0.8136986714683513),\n",
       " (30, 0.8108113231099193),\n",
       " (31, 0.8079818410005744),\n",
       " (32, 0.8052055679146205),\n",
       " (33, 0.8024798286320088),\n",
       " (34, 0.799800269497179),\n",
       " (35, 0.7971642498583381),\n",
       " (36, 0.794568227982998),\n",
       " (37, 0.7920098466066074),\n",
       " (38, 0.789485907390999),\n",
       " (39, 0.7869943212729456),\n",
       " (40, 0.7845329147841061),\n",
       " (41, 0.7820990617541793),\n",
       " (42, 0.7796900574044867),\n",
       " (43, 0.7773053607571365),\n",
       " (44, 0.7749432542942915),\n",
       " (45, 0.7726011860313782),\n",
       " (46, 0.7702778190993449),\n",
       " (47, 0.7679719640543011),\n",
       " (48, 0.7656821783992426),\n",
       " (49, 0.7634068719839527),\n",
       " (50, 0.7611456450361076)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_matrix = df_counts.pivot(index='user', columns='category', values='count')\n",
    "R_temp = view_matrix.fillna(0)\n",
    "mf = MF(R_temp, K=40, alpha=0.001, beta=0.02, iterations=50, verbose=True)\n",
    "mf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user  category  count\n",
      "0      90001        11      0\n",
      "1      90001        12      0\n",
      "2      90001        42      0\n",
      "3      90001        43      0\n",
      "4      90001        72      1\n",
      "...      ...       ...    ...\n",
      "63125  91220      1092      1\n",
      "63126  91220      1093      1\n",
      "63127  91220      1094      1\n",
      "63128  91220      1101      1\n",
      "63129  91220      1102      1\n",
      "\n",
      "[63130 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(view_matrix)\n",
    "view_matrix.fillna(0).to_csv('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6    \\\n",
      "0     1.885533  1.771663  1.934260  2.092536  1.846037  1.809250  2.033177   \n",
      "1     2.351098  2.185781  2.376757  2.530611  2.260632  2.216988  2.472687   \n",
      "2     1.952525  1.814063  1.993869  2.150045  1.884701  1.849649  2.088317   \n",
      "3     2.000778  1.855989  2.019426  2.182259  1.945041  1.910653  2.104952   \n",
      "4     2.085441  1.945690  2.137587  2.293316  2.055735  1.986186  2.226489   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1135  1.351928  1.236617  1.401715  1.561570  1.318477  1.306137  1.486042   \n",
      "1136  1.367222  1.239183  1.405272  1.562968  1.324696  1.297742  1.495311   \n",
      "1137  1.364708  1.238563  1.408184  1.561940  1.316284  1.288108  1.486972   \n",
      "1138  1.368120  1.225259  1.409565  1.559856  1.313930  1.288515  1.497884   \n",
      "1139  1.378413  1.241116  1.410571  1.564252  1.310349  1.290468  1.507035   \n",
      "\n",
      "           7         8         9    ...       413       414       415  \\\n",
      "0     2.067775  1.919615  1.864712  ...  1.533961  1.552928  1.444100   \n",
      "1     2.516836  2.366015  2.301601  ...  1.958541  1.975233  1.873054   \n",
      "2     2.141455  1.979194  1.932581  ...  1.584183  1.609966  1.495614   \n",
      "3     2.155725  2.017382  1.949574  ...  1.594925  1.606033  1.504657   \n",
      "4     2.281805  2.128214  2.073543  ...  1.748652  1.766536  1.665727   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1135  1.539367  1.392563  1.334168  ...  0.975682  1.002181  0.893785   \n",
      "1136  1.551574  1.397196  1.325153  ...  0.978669  1.004476  0.891586   \n",
      "1137  1.539197  1.390667  1.337337  ...  0.986738  1.015070  0.901207   \n",
      "1138  1.553419  1.393318  1.341458  ...  0.989183  1.001074  0.885321   \n",
      "1139  1.548792  1.384637  1.350672  ...  0.989504  0.997249  0.885924   \n",
      "\n",
      "           416       417       418       419       420       421       422  \n",
      "0     1.567069  1.842879  1.881994  1.822087  2.010429  1.648536  1.486688  \n",
      "1     1.986588  2.266633  2.287982  2.262031  2.455190  2.071919  1.928748  \n",
      "2     1.635188  1.854153  1.881098  1.871004  2.067079  1.704611  1.547868  \n",
      "3     1.615854  1.805626  1.868015  1.885433  2.070463  1.699118  1.556455  \n",
      "4     1.764725  2.109236  2.101567  2.068796  2.225364  1.863274  1.699504  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "1135  1.017346  1.213912  1.274690  1.262919  1.442202  1.083585  0.938969  \n",
      "1136  1.015972  1.185338  1.259826  1.251801  1.449259  1.084079  0.944220  \n",
      "1137  1.027479  1.180944  1.270596  1.269787  1.454880  1.092593  0.946529  \n",
      "1138  1.009739  1.197769  1.243437  1.263171  1.445006  1.080571  0.954062  \n",
      "1139  1.020577  1.199187  1.253254  1.268742  1.449570  1.095511  0.951141  \n",
      "\n",
      "[1140 rows x 423 columns]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "test_df222 = pd.DataFrame(mf.full_prediction())\n",
    "print(test_df222)\n",
    "print(test_df222.to_csv('test2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "남성 17\n",
      "      category      pred\n",
      "248  기타 자동차 용품  1.097760\n",
      "257         비데  1.122807\n",
      "245      세차 용품  1.132078\n",
      "258         욕조  1.136731\n",
      "232      기타 공구  1.142874\n",
      "..         ...       ...\n",
      "315     유아동 의류  2.036922\n",
      "7      남성 언더웨어  2.067775\n",
      "3      여성 언더웨어  2.092536\n",
      "303   위생/건강 용품  2.110703\n",
      "145        PSP  2.137379\n",
      "\n",
      "[423 rows x 2 columns]\n",
      "남성 22\n",
      "      category      pred\n",
      "248  기타 자동차 용품  1.425201\n",
      "257         비데  1.456655\n",
      "243        장난감  1.472348\n",
      "245      세차 용품  1.474575\n",
      "232      기타 공구  1.482565\n",
      "..         ...       ...\n",
      "315     유아동 의류  2.378765\n",
      "7      남성 언더웨어  2.399803\n",
      "3      여성 언더웨어  2.411278\n",
      "303   위생/건강 용품  2.430382\n",
      "145        PSP  2.460020\n",
      "\n",
      "[423 rows x 2 columns]\n",
      "남성 32\n",
      "      category      pred\n",
      "248  기타 자동차 용품  1.807416\n",
      "257         비데  1.833388\n",
      "245      세차 용품  1.849976\n",
      "232      기타 공구  1.852619\n",
      "243        장난감  1.857401\n",
      "..         ...       ...\n",
      "315     유아동 의류  2.743890\n",
      "7      남성 언더웨어  2.786192\n",
      "3      여성 언더웨어  2.789438\n",
      "303   위생/건강 용품  2.825898\n",
      "145        PSP  2.841536\n",
      "\n",
      "[423 rows x 2 columns]\n",
      "남성 42\n",
      "      category      pred\n",
      "248  기타 자동차 용품  1.251490\n",
      "257         비데  1.282549\n",
      "245      세차 용품  1.300240\n",
      "243        장난감  1.302005\n",
      "258         욕조  1.307879\n",
      "..         ...       ...\n",
      "315     유아동 의류  2.216301\n",
      "7      남성 언더웨어  2.231797\n",
      "3      여성 언더웨어  2.250161\n",
      "303   위생/건강 용품  2.268441\n",
      "145        PSP  2.305043\n",
      "\n",
      "[423 rows x 2 columns]\n",
      "남성 52\n",
      "      category      pred\n",
      "248  기타 자동차 용품  1.483920\n",
      "257         비데  1.511276\n",
      "258         욕조  1.525362\n",
      "245      세차 용품  1.528714\n",
      "243        장난감  1.532567\n",
      "..         ...       ...\n",
      "315     유아동 의류  2.437150\n",
      "7      남성 언더웨어  2.459358\n",
      "3      여성 언더웨어  2.470555\n",
      "303   위생/건강 용품  2.498419\n",
      "145        PSP  2.524961\n",
      "\n",
      "[423 rows x 2 columns]\n",
      "남성 62\n",
      "      category      pred\n",
      "248  기타 자동차 용품  1.607640\n",
      "257         비데  1.635830\n",
      "258         욕조  1.648577\n",
      "245      세차 용품  1.658768\n",
      "243        장난감  1.663655\n",
      "..         ...       ...\n",
      "326      교재/서적  2.559126\n",
      "3      여성 언더웨어  2.572996\n",
      "7      남성 언더웨어  2.594437\n",
      "303   위생/건강 용품  2.619483\n",
      "145        PSP  2.651070\n",
      "\n",
      "[423 rows x 2 columns]\n",
      "남성 72\n",
      "      category      pred\n",
      "248  기타 자동차 용품  1.826204\n",
      "257         비데  1.862659\n",
      "245      세차 용품  1.866819\n",
      "243        장난감  1.877649\n",
      "258         욕조  1.879086\n",
      "..         ...       ...\n",
      "315     유아동 의류  2.788587\n",
      "7      남성 언더웨어  2.802125\n",
      "3      여성 언더웨어  2.811487\n",
      "303   위생/건강 용품  2.835234\n",
      "145        PSP  2.885099\n",
      "\n",
      "[423 rows x 2 columns]\n",
      "여성 12\n",
      "      category      pred\n",
      "248  기타 자동차 용품  1.483340\n",
      "257         비데  1.510412\n",
      "245      세차 용품  1.519721\n",
      "258         욕조  1.525370\n",
      "232      기타 공구  1.532110\n",
      "..         ...       ...\n",
      "315     유아동 의류  2.428131\n",
      "7      남성 언더웨어  2.460500\n",
      "3      여성 언더웨어  2.472952\n",
      "303   위생/건강 용품  2.477584\n",
      "145        PSP  2.517965\n",
      "\n",
      "[423 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "testers = [\n",
    "    {\n",
    "        'user_id': 90001,\n",
    "        'gender': '남성',\n",
    "        'age': 17\n",
    "    }, {\n",
    "        'user_id': 90081,\n",
    "        'gender': '남성',\n",
    "        'age': 22\n",
    "    }, {\n",
    "        'user_id': 90161,\n",
    "        'gender': '남성',\n",
    "        'age': 32\n",
    "    }, {\n",
    "        'user_id': 90221,\n",
    "        'gender': '남성',\n",
    "        'age': 42\n",
    "    }, {\n",
    "        'user_id': 90281,\n",
    "        'gender': '남성',\n",
    "        'age': 52\n",
    "    }, {\n",
    "        'user_id': 90341,\n",
    "        'gender': '남성',\n",
    "        'age': 62\n",
    "    }, {\n",
    "        'user_id': 90401,\n",
    "        'gender': '남성',\n",
    "        'age': 72\n",
    "    }, {\n",
    "        'user_id': 90501,\n",
    "        'gender': '여성',\n",
    "        'age': 12\n",
    "    }\n",
    "]\n",
    "\n",
    "for tester in testers:\n",
    "    print(tester['gender'], tester['age'])\n",
    "    temp_data = dict()\n",
    "    for category in R_temp:\n",
    "        temp_data[category] = mf.get_one_prediction(tester['user_id'], category)\n",
    "\n",
    "    recomm_df = pd.DataFrame.from_dict([temp_data]).T.reset_index().rename(columns={'index': 'category', 0: 'pred'})\n",
    "    # print(recomm_df)\n",
    "    temp_recomm_df = recomm_df.sort_values(by='pred', ascending=True)\n",
    "    temp_recomm_df['category'] = temp_recomm_df['category'].map(lambda x: category_dict[x])\n",
    "    print(temp_recomm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2.092536\n",
      "1       2.530611\n",
      "2       2.150045\n",
      "3       2.182259\n",
      "4       2.293316\n",
      "          ...   \n",
      "1135    1.561570\n",
      "1136    1.562968\n",
      "1137    1.561940\n",
      "1138    1.559856\n",
      "1139    1.564252\n",
      "Name: 3, Length: 1140, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(test_df222[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     category      pred\n",
      "0           1  0.695231\n",
      "1           2  0.621502\n",
      "2           3  0.741396\n",
      "3           4  0.696715\n",
      "4          11  0.884899\n",
      "..        ...       ...\n",
      "418      1092  0.754905\n",
      "419      1093  0.776601\n",
      "420      1094  0.776212\n",
      "421      1101  0.725899\n",
      "422      1102  0.757372\n",
      "\n",
      "[423 rows x 2 columns]\n",
      "     category      pred\n",
      "9       여성 구두  0.601747\n",
      "24     여성용 가방  0.609848\n",
      "12     여성 실내화  0.614230\n",
      "11     여성 운동화  0.615850\n",
      "10    여성 캐주얼화  0.618955\n",
      "..        ...       ...\n",
      "339       축구화  0.976315\n",
      "20   기타 신발 용품  1.007490\n",
      "328       수영모  1.018621\n",
      "338       축구공  1.083338\n",
      "5       남성 하의  1.131398\n",
      "\n",
      "[423 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def recom_category(user_id, n_items=20):\n",
    "    for category in view_matrix:\n",
    "        user_category.loc[category] = IBCF_model(user_id, category)\n",
    "    return user_category.reset_index().rename(columns={user_id: 'pred'})\n",
    "\n",
    "def IBCF_model(user, category):\n",
    "    if category in item_similarity:\n",
    "        sim_scores = item_similarity[category]\n",
    "        user_viewing = user_category.T\n",
    "        non_viewing_idx = user_viewing[user_viewing.isnull()].index\n",
    "        user_viewing = user_viewing.dropna()\n",
    "        sim_scores = sim_scores.drop(non_viewing_idx)\n",
    "        pred_viewing = np.dot(sim_scores, user_viewing) / sim_scores.sum()\n",
    "    else:\n",
    "        pred_viewing = 0\n",
    "    return pred_viewing\n",
    "\n",
    "user_category = view_matrix.loc[90021].copy()\n",
    "\n",
    "pred_user_category = recom_category(user_id=90021, n_items=423)\n",
    "# recommend_category = pd.DataFrame(pred_user_category).index.map(lambda x: category_dict[x])\n",
    "print(pred_user_category)\n",
    "\n",
    "temp_temp_temp = pred_user_category.sort_values(by='pred')\n",
    "temp_temp_temp['category'] = temp_temp_temp['category'].map(lambda x: category_dict[x])\n",
    "print(temp_temp_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     category  pred_IBCF   pred_MF      pred\n",
      "9       여성 구두   0.601747  2.228518  0.764424\n",
      "12     여성 실내화   0.614230  2.139266  0.766734\n",
      "10    여성 캐주얼화   0.618955  2.105933  0.767653\n",
      "11     여성 운동화   0.615850  2.165405  0.770805\n",
      "352       배구공   0.634319  2.009606  0.771848\n",
      "..        ...        ...       ...       ...\n",
      "339       축구화   0.976315  2.071808  1.085865\n",
      "20   기타 신발 용품   1.007490  2.188747  1.125616\n",
      "328       수영모   1.018621  2.326583  1.149418\n",
      "338       축구공   1.083338  2.031960  1.178200\n",
      "5       남성 하의   1.131398  2.184143  1.236672\n",
      "\n",
      "[423 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "merged = pd.merge(pred_user_category, recomm_df, left_on='category', right_on='category', how='outer', suffixes=['_IBCF', '_MF'])\n",
    "merged['pred'] = merged['pred_IBCF'] * 0.9 + merged['pred_MF'] * 0.1\n",
    "merged['category'] = merged['category'].map(lambda x: category_dict[x])\n",
    "print(merged.sort_values(by='pred'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "dataframes = pd.read_pickle('recom_data/user_category_dummy.pkl')\n",
    "\n",
    "df_counts = dataframes['view_counts']\n",
    "df_users = dataframes['users']\n",
    "\n",
    "with open('recom_data/category_dict.pkl', 'rb') as handle:\n",
    "    category_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# shuffle 라이브러리 사용 -> 전체 평가를 기준으로 랜덤 추출\n",
    "# 극단적인 경우, 특정 사용자의 모튼 평가가 train_set으로 들어갈 수도 있음\n",
    "\n",
    "TRAIN_SIZE = 0.75 \n",
    "# train_set 을 75%로 지정\n",
    "view_counts = shuffle(df_counts, random_state=1)\n",
    "# ratings를 섞어줌 -> 사용자-영화-평점이 1세트\n",
    "cutoff = int(TRAIN_SIZE * len(view_counts))\n",
    "# 전체 데이터 중 train_set의 개수를 계산\n",
    "view_train = view_counts.iloc[:cutoff]\n",
    "view_test = view_counts.iloc[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD, Adamax\n",
    "\n",
    "# 변수 초기화\n",
    "K = 200\n",
    "mu = view_train['count'].mean() # 전체 평균\n",
    "M = view_counts.user.max() + 1  # Number of users\n",
    "N = view_counts.category.max() + 1 # Number of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining RMSE measure\n",
    "def RMSE(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras model\n",
    "user = Input(shape=(1, ))\n",
    "item = Input(shape=(1, ))\n",
    "P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)\n",
    "Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)\n",
    "user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)\n",
    "item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)\n",
    "\n",
    "# Concatenate layers\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation\n",
    "P_embedding = Flatten()(P_embedding)\n",
    "Q_embedding = Flatten()(Q_embedding)\n",
    "user_bias = Flatten()(user_bias)\n",
    "item_bias = Flatten()(item_bias)\n",
    "R = Concatenate()([P_embedding, Q_embedding, user_bias, item_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 200)       18244200    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 200)       220600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 1)         91221       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 1)         1103        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 200)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1)            0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 1)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 402)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         825344      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 2048)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 19,907,269\n",
      "Trainable params: 19,907,269\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network\n",
    "R = Dense(2048)(R)\n",
    "R = Activation('linear')(R)\n",
    "R = Dense(256)(R)\n",
    "R = Activation('linear')(R)\n",
    "R = Dense(1)(R)\n",
    "\n",
    "model = Model(inputs = [user, item], outputs=R)\n",
    "model.compile(\n",
    "    loss=RMSE,\n",
    "    optimizer=SGD(),\n",
    "    #opto,ozer=Adamax(),\n",
    "    metrics=[RMSE]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee33\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 31s 325ms/step - RMSE: 1.1581 - loss: 152.9299 - val_loss: 150.0474 - val_RMSE: 1.1581\n",
      "Epoch 2/65\n",
      "94/94 [==============================] - 28s 301ms/step - RMSE: 1.1578 - loss: 147.3285 - val_loss: 144.5523 - val_RMSE: 1.1579\n",
      "Epoch 3/65\n",
      "94/94 [==============================] - 27s 290ms/step - RMSE: 1.1578 - loss: 141.9338 - val_loss: 139.2601 - val_RMSE: 1.1577\n",
      "Epoch 4/65\n",
      "94/94 [==============================] - ETA: 0s - RMSE: 1.1576 - loss: 136.7383"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-9a8d642709c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     validation_data = (\n\u001b[0;32m      8\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mview_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mview_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m               return_dict=True)\n\u001b[0m\u001b[0;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model fitting\n",
    "result = model.fit(\n",
    "    x = [view_train.user.values, view_train.category.values],\n",
    "    y = view_train['count'].values - mu,\n",
    "    epochs = 65,\n",
    "    batch_size = 512,\n",
    "    validation_data = (\n",
    "        [view_train.user.values, view_train.category.values],\n",
    "        view_train['count'].values - mu\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(result.history['RMSE'], label=\"Train RMSE\")\n",
    "plt.plot(result.history['val_RMSE'], label=\"Test RMSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ids = view_test.user.values\n",
    "# category_ids = view_test.category.values\n",
    "# user = np.array([90421] * len(category_dict))\n",
    "# categories = np.array(list(category_dict.keys()))\n",
    "# predictions = model.predict([user, categories]) + mu\n",
    "\n",
    "# print(category_dict[categories[predictions.argmax()]])\n",
    "# print(predictions.argmax())\n",
    "# print(\"Actuals: \\n\", view_test[0:6])\n",
    "# print()\n",
    "# print(\"Predictions: \\n\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"testmodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dataframes = pd.read_pickle('recom_data/user_category_dummy3.pkl')\n",
    "\n",
    "df_counts = dataframes['view_counts']\n",
    "df_users = dataframes['users']\n",
    "\n",
    "with open('recom_data/category_dict.pkl', 'rb') as handle:\n",
    "    category_dict = pickle.load(handle)\n",
    "\n",
    "item_similarity = pd.read_pickle('recom_data/item_similarity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# shuffle 라이브러리 사용 -> 전체 평가를 기준으로 랜덤 추출\n",
    "# 극단적인 경우, 특정 사용자의 모튼 평가가 train_set으로 들어갈 수도 있음\n",
    "\n",
    "TRAIN_SIZE = 0.75 \n",
    "# train_set 을 75%로 지정\n",
    "view_counts = shuffle(df_counts, random_state=1)\n",
    "# ratings를 섞어줌 -> 사용자-영화-평점이 1세트\n",
    "cutoff = int(TRAIN_SIZE * len(view_counts))\n",
    "# 전체 데이터 중 train_set의 개수를 계산\n",
    "view_train = view_counts.iloc[:cutoff]\n",
    "view_test = view_counts.iloc[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_gender(gender):\n",
    "    if gender == 'M':\n",
    "        return 1\n",
    "    elif gender == 'F':\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def convert_age(age):\n",
    "    if age == 100:\n",
    "        return 0\n",
    "    else:\n",
    "        return age // 10\n",
    "    \n",
    "users = df_users.copy()\n",
    "users['age'] = users['age'].apply(convert_age)\n",
    "users['gender'] = users['gender'].apply(convert_gender)\n",
    "\n",
    "A, G = 8, 3\n",
    "train_age = pd.merge(view_train, users, on='user')['age']\n",
    "test_age = pd.merge(view_test, users, on='user')['age']\n",
    "\n",
    "train_gender = pd.merge(view_train, users, on='user')['gender']\n",
    "test_gender = pd.merge(view_test, users, on='user')['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD, Adamax\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation\n",
    "\n",
    "# 변수 초기화\n",
    "K = 200\n",
    "mu = view_train['count'].mean() # 전체 평균\n",
    "M = view_counts.user.max() + 1  # Number of users\n",
    "N = view_counts.category.max() + 1 # Number of movies\n",
    "\n",
    "# Defining RMSE measure\n",
    "def RMSE(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "# Keras model\n",
    "user = Input(shape=(1, ))\n",
    "item = Input(shape=(1, ))\n",
    "P_embedding = Embedding(M, K, embeddings_regularizer=l2())(user)\n",
    "Q_embedding = Embedding(N, K, embeddings_regularizer=l2())(item)\n",
    "user_bias = Embedding(M, 1, embeddings_regularizer=l2())(user)\n",
    "item_bias = Embedding(N, 1, embeddings_regularizer=l2())(item)\n",
    "\n",
    "age = Input(shape=(1, ))\n",
    "age_embedding = Embedding(A, 3, embeddings_regularizer=l2())(age)\n",
    "gender = Input(shape=(1, ))\n",
    "gender_embedding = Embedding(G, 3, embeddings_regularizer=l2())(gender)\n",
    "\n",
    "P_embedding = Flatten()(P_embedding)\n",
    "Q_embedding = Flatten()(Q_embedding)\n",
    "user_bias = Flatten()(user_bias)\n",
    "item_bias = Flatten()(item_bias)\n",
    "\n",
    "age_layer = Flatten( )(age_embedding)\n",
    "gender_layer = Flatten( )(gender_embedding)\n",
    "\n",
    "R = Concatenate( )([P_embedding, Q_embedding, user_bias, item_bias, gender_layer, age_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_36 (Embedding)        (None, 1, 200)       18244200    input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_37 (Embedding)        (None, 1, 200)       220600      input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_38 (Embedding)        (None, 1, 1)         91221       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_39 (Embedding)        (None, 1, 1)         1103        input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_41 (Embedding)        (None, 1, 3)         9           input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_40 (Embedding)        (None, 1, 3)         24          input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_36 (Flatten)            (None, 200)          0           embedding_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_37 (Flatten)            (None, 200)          0           embedding_37[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_38 (Flatten)            (None, 1)            0           embedding_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_39 (Flatten)            (None, 1)            0           embedding_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_41 (Flatten)            (None, 3)            0           embedding_41[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_40 (Flatten)            (None, 3)            0           embedding_40[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 408)          0           flatten_36[0][0]                 \n",
      "                                                                 flatten_37[0][0]                 \n",
      "                                                                 flatten_38[0][0]                 \n",
      "                                                                 flatten_39[0][0]                 \n",
      "                                                                 flatten_41[0][0]                 \n",
      "                                                                 flatten_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 2048)         837632      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 2048)         0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 256)          524544      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 256)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            257         activation_13[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 19,919,590\n",
      "Trainable params: 19,919,590\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network\n",
    "R = Dense(2048)(R)\n",
    "R = Activation('linear')(R)\n",
    "R = Dense(256)(R)\n",
    "R = Activation('linear')(R)\n",
    "R = Dense(1)(R)\n",
    "\n",
    "model = Model(inputs = [user, item, gender, age], outputs=R)\n",
    "model.compile(\n",
    "    loss=RMSE,\n",
    "    optimizer=SGD(),\n",
    "    #opto,ozer=Adamax(),\n",
    "    metrics=[RMSE]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee33\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 23s 249ms/step - loss: 152.9909 - RMSE: 1.1735 - val_RMSE: 1.1663 - val_loss: 150.1321\n",
      "Epoch 2/40\n",
      "93/93 [==============================] - 23s 249ms/step - loss: 147.4462 - RMSE: 1.1728 - val_RMSE: 1.1662 - val_loss: 144.6918\n",
      "Epoch 3/40\n",
      "93/93 [==============================] - 23s 251ms/step - loss: 142.1043 - RMSE: 1.1728 - val_RMSE: 1.1661 - val_loss: 139.4501\n",
      "Epoch 4/40\n",
      "93/93 [==============================] - 23s 248ms/step - loss: 136.9572 - RMSE: 1.1727 - val_RMSE: 1.1660 - val_loss: 134.3999\n",
      "Epoch 5/40\n",
      "26/93 [=======>......................] - ETA: 15s - loss: 133.7411 - RMSE: 1.1714"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-40bcae6ccea4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     validation_data = (\n\u001b[0;32m      8\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mview_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_gender\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_age\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mview_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model fitting\n",
    "result = model.fit(\n",
    "    x = [view_train.user.values, view_train.category.values, train_gender.values, train_age.values],\n",
    "    y = view_train['count'].values - mu,\n",
    "    epochs = 40,\n",
    "    batch_size = 512,\n",
    "    validation_data = (\n",
    "        [view_test.user.values, view_test.category.values, test_gender.values, test_age.values],\n",
    "        view_test['count'].values - mu,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
